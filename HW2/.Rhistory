# Cleaning workspace / loading libraries
rm(list=ls())
gc()
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(textdata)
library(wordcloud)
library(reshape2)
library(wordcloud)
shakespeare_corpus <- read_csv('shakespeare_gutenberg.csv')
View(shakespeare_corpus)
eme_stop_words <- tibble(word=c("thee", "thou", "hath", "thy", "thine", "ye",
"tis"))
book <- gutenberg_download(shakespeare_corpus$'Gutenberg ID'[1])
book <- book %>%
add_row(gutenberg_download(shakespeare_corpus$'Gutenberg ID'[2]))
for(i in 3:nrow(shakespeare_corpus)) {
book <- book %>%
add_row(gutenberg_download(shakespeare_corpus$'Gutenberg ID'[i]))
}
shakespeare_corpus <- read_csv('shakespeare_gutenberg.csv')
# Load Libraries
library(tidyverse)
library(gutenbergr)
library(tidytext)
dickens_list <- read_csv('dickens_gutenberg.csv')
mirror <- 'http://gutenberg.readingroo.ms/'
book <- gutenberg_download(dickens_list$'Gutenberg ID'[1], mirror = mirror)
for(i in 2:nrow(dickens_list)) {
book <- book %>% add_row(gutenberg_download(dickens_list$`Gutenberg ID`[i],
mirror = mirror))
}
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID"))
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
# Retrieving / applying associated sentiments of words using "bing" lexicon
bing_sentiments <- get_sentiments("bing")
tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Type) %>%
count(sentiment)
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID"))
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
View(total_words)
# Retrieving / applying associated sentiments of words using "bing" lexicon
bing_sentiments <- get_sentiments("bing")
tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Type) %>%
count(sentiment)
tidy_dickens %>% inner_join(bing_sentiments) %>%
count(sentiment)
View(tidy_dickens)
View(total_words)
View(tidy_dickens)
tidy_dickens %>% inner_join(bing_sentiments) %>%
count(sentiment)
View(bing_sentiments)
View(tidy_dickens)
tidy_dickens %>% inner_join(bing_sentiments) %>%
count(sentiment)
# Graphical analysis of gathered sentiment information
tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Type) %>%
count(sentiment) %>% ggplot(aes(Type, n, fill=sentiment)) +
geom_col(position='dodge')
tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment)
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID"))
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
# Retrieving / applying associated sentiments of words using "bing" lexicon
bing_sentiments <- get_sentiments("bing")
tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment)
tidy_dickens %>% inner_join(bing_sentiments)
View(dickens_list)
View(dickens_list)
View(tidy_dickens)
tidy_dickens %>% inner_join(bing_sentiments)
View(tidy_dickens)
tidy_dickens <- tidy_dickens %>% inner_join(bing_sentiments)
View(tidy_dickens)
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID"))
tidy_dickens %>% inner_join(bing_sentiments)
tidy_dickens <- tidy_dickens %>% inner_join(bing_sentiments)
# Graphical analysis of gathered sentiment information
tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% ggplot(aes(Type, n, fill=sentiment)) +
geom_col(position='dodge')
# Graphical analysis of gathered sentiment information
tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')
# Graphical analysis of gathered sentiment information
tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
tidy_dickens <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name)
# Graphical analysis of gathered sentiment information
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
# Contingency table on sentiment information - widening for multi. relationships
ct <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% pivot_wider(names_from = sentiment, values_from = n) %>%
column_to_rownames(var = "Name")
View(ct)
# Load Libraries
library(tidyverse)
library(gutenbergr)
library(tidytext)
dickens_list <- read_csv('dickens_gutenberg.csv')
mirror <- 'http://gutenberg.readingroo.ms/'
book <- gutenberg_download(dickens_list$'Gutenberg ID'[1], mirror = mirror)
for(i in 2:nrow(dickens_list)) {
book <- book %>% add_row(gutenberg_download(dickens_list$`Gutenberg ID`[i],
mirror = mirror))
}
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID"))
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
# Sentiment Analysis - Bing
bing_sentiments <- get_sentiments("bing")
tidy_dickens <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name)
# Chi-sq test
chSq <- chisq.test(ct)
View(ct)
View(chSq)
chSq
# Analysis of chi-square test - expected values and implications regarding
# relationships
chSq$observed
chSq$expected
chSq$stdres
# Pivoting to the use of "afinn" lexicon to determine sentiment by word weight
afinn_sentiments <- get_sentiments("afinn")
tidy_dickens_afinn <- tidy_dickens %>%
inner_join(afinn_sentiments) %>% inner_join(bing_sentiments)
tidy_dickens_afinn %>% group_by(Name) %>%
summarize(value = mean(value))
View(tidy_dickens_afinn)
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_violin()
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_point()
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_boxplot()
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_dotplot()
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_linerange()
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_vline()
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_area()
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_col()
tidy_dickens_afinn %>% ggplot(aes(Name, value)) + geom_violin()
# Graphs
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
# Contingency table
ct <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% pivot_wider(names_from = sentiment, values_from = n) %>%
column_to_rownames(var = "Name")
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID")) %>%
anti_join(stop_words)
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
# Sentiment Analysis - Bing
bing_sentiments <- get_sentiments("bing")
tidy_dickens <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name)
# Graphs
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
source("~/DATA312/HW2/HW2C.R")
# Load Libraries
library(tidyverse)
library(gutenbergr)
library(tidytext)
dickens_list <- read_csv('dickens_gutenberg.csv')
View(dickens_list)
book <- gutenberg_download(dickens_list$'Gutenberg ID'[1])
for(i in 2:nrow(dickens_list)) {
book <- book %>% add_row(gutenberg_download(dickens_list$`Gutenberg ID`[i]))
}
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID")) %>%
anti_join(stop_words)
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
# Sentiment Analysis - Bing
bing_sentiments <- get_sentiments("bing")
tidy_dickens <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name)
# Graphs
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
# Graphs
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word =")
rm(list=ls())
gc()
rm(list=ls())
gc()
rm(list=ls())
gc()
rm(list=ls())
gc()
rm(list=ls())
gc()
rm(list=ls())
gc()
rm(list=ls())
gc()
rm(list=ls())
gc()
# Graphs
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
dickens_list <- read_csv('dickens_gutenberg.csv')
mirror <- 'http://gutenberg.readingroo.ms/'
book <- gutenberg_download(dickens_list$'Gutenberg ID'[1], mirror = mirror)
for(i in 2:nrow(dickens_list)) {
book <- book %>% add_row(gutenberg_download(dickens_list$`Gutenberg ID`[i],
mirror = mirror))
}
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID")) %>%
anti_join(stop_words)
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
# Sentiment Analysis - Bing
bing_sentiments <- get_sentiments("bing")
tidy_dickens <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name)
# Graphs
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
# Contingency table
ct <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% pivot_wider(names_from = sentiment, values_from = n) %>%
column_to_rownames(var = "Name")
# Chi-sq test
chSq <- chisq.test(ct)
View(bing_sentiments)
chSq
chSq$observed
# Contingency table
ct <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% pivot_wider(names_from = sentiment, values_from = n) %>%
column_to_rownames(var = "Name")
# Chi-sq test
chSq <- chisq.test(ct)
View(ct)
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID"))
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID"))
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
# Sentiment Analysis - Bing
bing_sentiments <- get_sentiments("bing")
tidy_dickens <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name)
# Graphs
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
# Contingency table
ct <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% pivot_wider(names_from = sentiment, values_from = n) %>%
column_to_rownames(var = "Name")
# Graphs
tidy_dickens %>%
count(sentiment) %>% ggplot(aes(Name, n, fill=sentiment)) +
geom_col(position='dodge')  +
labs(title="Dickens Sentiment Distributions - Bing Lexicon",
x = "Work",
y = "Word Count")
# Contingency table
ct <- tidy_dickens %>% inner_join(bing_sentiments) %>% group_by(Name) %>%
count(sentiment) %>% pivot_wider(names_from = sentiment, values_from = n) %>%
column_to_rownames(var = "Name")
words_by_rank <- tidy_dickens %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
rm(list=ls())
gc()
# Load Libraries
library(tidyverse)
library(gutenbergr)
library(tidytext)
library(reshape2)
library(wordcloud)
dickens_list <- read_csv('dickens_gutenberg.csv')
mirror <- 'http://gutenberg.readingroo.ms/'
book <- gutenberg_download(dickens_list$`Gutenberg ID`[1],
mirror = mirror) %>% slice(dickens_list$start[1]:n())
for(i in 2:nrow(dickens_list)) {
book <- book %>% add_row(gutenberg_download(dickens_list$`Gutenberg ID`[i],
mirror = mirror) %>% slice(dickens_list$start[i]:n()))
}
tidy_dickens <- book %>% unnest_tokens(word, text) %>%
count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID"))
tidy_dickens_no_sw <- tidy_dickens %>% anti_join(stop_words)
total_words <- tidy_dickens %>% group_by(Name) %>% summarize(total=sum(n))
dickens_words <- left_join(tidy_dickens, total_words)
bing_sentiments <- get_sentiments("bing")
words_by_rank <- tidy_dickens %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
View(words_by_rank)
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10()
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10()
lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10() +
geom_abline(intercept = lfit$coefficients["(Intercept)"],
slope = lfit$coefficients["log10(rank)"])
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10() +
geom_abline(intercept = lfit$coefficients["(Intercept)"],
slope = lfit$coefficients["log10(rank)"])
dickens_tf_idf <- dickens_words %>% bind_tf_idf(word, Name, n)
View(dickens_tf_idf)
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10() +
geom_abline(intercept = lfit$coefficients["(Intercept)"],
slope = lfit$coefficients["log10(rank)"])
dickens_tf_idf <- dickens_words %>% bind_tf_idf(word, Name, n) %>%
arrange(desc(tf_idf))
View(dickens_tf_idf)
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10() +
geom_abline(intercept = lfit$coefficients["(Intercept)"],
slope = lfit$coefficients["log10(rank)"])
dickens_tf_idf <- dickens_words %>% bind_tf_idf(word, Name, n) %>%
arrange(desc(tf_idf))
dickens_tf_idf %>% group_by(Name) %>% slice_max(tf_idf, n = 10)
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10() +
geom_abline(intercept = lfit$coefficients["(Intercept)"],
slope = lfit$coefficients["log10(rank)"])
dickens_tf_idf <- dickens_words %>% bind_tf_idf(word, Name, n) %>%
arrange(desc(tf_idf))
dickens_tf_idf %>% group_by(Name) %>% slice_max(tf_idf, n = 10) %>%
ggplot(aes(tf_idf, word, fill=Name)) + geom_col(show.legend = FALSE) +
facet_wrap(~Name)
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10() +
geom_abline(intercept = lfit$coefficients["(Intercept)"],
slope = lfit$coefficients["log10(rank)"])
dickens_tf_idf <- dickens_words %>% bind_tf_idf(word, Name, n) %>%
arrange(desc(tf_idf))
dickens_tf_idf %>% group_by(Name) %>% slice_max(tf_idf, n = 10) %>%
ggplot(aes(tf_idf, word, fill=Name)) + geom_col(show.legend = FALSE) +
facet_wrap(~Name, scales = "free")
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10() +
geom_abline(intercept = lfit$coefficients["(Intercept)"],
slope = lfit$coefficients["log10(rank)"])
dickens_tf_idf <- dickens_words %>% bind_tf_idf(word, Name, n) %>%
arrange(desc(tf_idf))
dickens_tf_idf %>% group_by(Name) %>% slice_max(tf_idf, n = 10) %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill=Name)) +
geom_col(show.legend = FALSE) + facet_wrap(~Name, scales = "free")
tidy_dickens_no_sw %>% with(wordcloud(word, n, max.words=100))
words_by_rank <- dickens_words %>% group_by(Name) %>%
mutate(rank = row_number(), frequency = n/total) %>% ungroup()
lfit <- lm(log10(frequency) ~ log10(rank), data = words_by_rank)
words_by_rank %>% ggplot(aes(rank, frequency, color = Name)) +
geom_line(show.legend = FALSE) + scale_x_log10() + scale_y_log10() +
geom_abline(intercept = lfit$coefficients["(Intercept)"],
slope = lfit$coefficients["log10(rank)"])
dickens_tf_idf <- dickens_words %>% bind_tf_idf(word, Name, n) %>%
arrange(desc(tf_idf))
dickens_tf_idf %>% group_by(Name) %>% slice_max(tf_idf, n = 10) %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill=Name)) +
geom_col(show.legend = FALSE) + facet_wrap(~Name, scales = "free")
# Comment: This is undoubtedly drastic, but as far as I can tell, it works.
# Do let me know if I've made any errors.
for(i in 1:nrow(dickens_list)) {
gutenberg_download(dickens_list$`Gutenberg ID`[i],
mirror = mirror) %>% slice(dickens_list$start[i]:n()) %>%
unnest_tokens(word, text) %>% count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID")) %>%
anti_join(stop_words) %>% with(wordcloud(word, n, max.words=100))
}
knitr::opts_chunk$set(fig.height = FALSE, fig.width = FALSE, fig.align = FALSE)
# Comment: This is undoubtedly drastic, but as far as I can tell, it works.
# Do let me know if I've made any errors.
for(i in 1:nrow(dickens_list)) {
gutenberg_download(dickens_list$`Gutenberg ID`[i],
mirror = mirror) %>% slice(dickens_list$start[i]:n()) %>%
unnest_tokens(word, text) %>% count(gutenberg_id, word, sort = TRUE) %>%
inner_join(dickens_list, by = c(gutenberg_id = "Gutenberg ID")) %>%
anti_join(stop_words) %>% with(wordcloud(word, n, max.words=100))
}
